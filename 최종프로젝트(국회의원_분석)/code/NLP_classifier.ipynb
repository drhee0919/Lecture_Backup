{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_classifier.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Mp7thUVYWD2c_S_4TuJjsDYKu8sAXaL9","authorship_tag":"ABX9TyM4Aejjs0Wk4aaKhduy1F3T"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"jiAJGNqhJE3A","colab_type":"code","colab":{}},"source":["## run_nltk\n","\n","from konlpy.tag import Twitter\n","import nltk\n","\n","twitter = Twitter()\n","\n","print(twitter.morphs(u'한글형태소분석기 테스트 중 입니다')) # ??\n","print(twitter.nouns(u'한글형태소분석기 테스트 중 입니다!')) #명사\n","print(twitter.pos(u'한글형태소분석기 테스트 중 입니다.')) #형태소\n","\n","def read_data(filename):\n","    with open(filename, 'r') as f:\n","        data = [line.split('\\t') for line in f.read().splitlines()]\n","    return data\n","\n","def tokenize(doc):\n","  # norm, stem은 optional\n","  return ['/'.join(t) for t in twitter.pos(doc, norm=True, stem=True)]\n","\n","def term_exists(doc):\n","    return {'exists({})'.format(word): (word in set(doc)) for word in selected_words}\n","\n","# 트래이닝 데이터와 테스트 데이터를 읽기\n","train_data = read_data('data/ratings_train.txt')\n","test_data = read_data('data/ratings_test.txt')\n","\n","# row, column의 수가 제대로 읽혔는지 확인\n","print(len(train_data))      # nrows: 150000\n","print(len(train_data[0]))   # ncols: 3\n","print(len(test_data))       # nrows: 50000\n","print(len(test_data[0]))     # ncols: 3\n","\n","# 형태소 분류\n","train_docs = [(tokenize(row[1]), row[2]) for row in train_data[1:]]\n","test_docs = [(tokenize(row[1]), row[2]) for row in test_data[1:]]\n","\n","#Training data의 token 모으기\n","tokens = [t for d in train_docs for t in d[0]]\n","print(len(tokens))\n","\n","# Load tokens with nltk.Text()\n","text = nltk.Text(tokens, name='NMSC')\n","print(text.vocab().most_common(10))\n","\n","# 텍스트간의 연어 빈번하게 등장하는 단어 구하기\n","text.collocations()\n","\n","\n","# term이 존재하는지에 따라서 문서를 분류\n","selected_words = [f[0] for f in text.vocab().most_common(2000)] # 여기서는 최빈도 단어 2000개를 피쳐로 사용\n","train_docs = train_docs[:10000] # 시간 단축을 위한 꼼수로 training corpus의 일부만 사용할 수 있음\n","train_xy = [(term_exists(d), c) for d, c in train_docs]\n","test_xy = [(term_exists(d), c) for d, c in test_docs]\n","\n","# nltk의 NaiveBayesClassifier으로 데이터를 트래이닝 시키고, test 데이터로 확인\n","classifier = nltk.NaiveBayesClassifier.train(train_xy) #Naive Bayes classifier 적용\n","print(nltk.classify.accuracy(classifier, test_xy))\n","# => 0.80418\n","\n","classifier.show_most_informative_features(10)\n","\n","\n","\n","\n","\n","\n","\n","#nltk.polarity_scores(\"i love you\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBCCY5YeLkfd","colab_type":"code","colab":{}},"source":["## doc2vec_run\n","\n","from collections import namedtuple\n","from gensim.models import doc2vec\n","from konlpy.tag import Twitter\n","import multiprocessing\n","from pprint import pprint\n","from gensim.models import Doc2Vec\n","from sklearn.linear_model import LogisticRegression\n","import numpy\n","import pickle\n","\n","twitter = Twitter()\n","\n","def read_data(filename):\n","    with open(filename, 'r') as f:\n","        data = [line.split('\\t') for line in f.read().splitlines()]\n","    return data\n","\n","def tokenize(doc):\n","  # norm, stem은 optional\n","  return ['/'.join(t) for t in twitter.pos(doc, norm=True, stem=True)]\n","\n","\n","# 실제 구동 데이터를 읽기\n","run_data = read_data('data/ratings_run.txt')\n","\n","# 형태소 분류\n","run_docs = [(tokenize(row[1]), row[2]) for row in run_data[1:]]\n","\n","# doc2vec 에서 필요한 데이터 형식으로 변경\n","TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n","tagged_run_docs = [TaggedDocument(d, [c]) for d, c in run_docs]\n","\n","# load train data\n","doc_vectorizer = Doc2Vec.load('model/doc2vec.model')\n","\n","# 분류를 위한 피쳐 생성\n","run_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_run_docs]\n","run_y = [doc.tags[0] for doc in tagged_run_docs]\n","\n","# load the model from disk\n","filename = 'model/finalized_model.sav'\n","\n","# 실제 분류 확인\n","loaded_model = pickle.load(open(filename, 'rb'))\n","print(loaded_model.predict(run_x[0].reshape(1, -1)))\n","print(loaded_model.predict(run_x[1].reshape(1, -1)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTfBOskYLwwO","colab_type":"code","colab":{}},"source":["## doc2vec_train\n","from collections import namedtuple\n","from gensim.models import doc2vec\n","from konlpy.tag import Twitter\n","import multiprocessing\n","from pprint import pprint\n","\n","twitter = Twitter()\n","\n","def read_data(filename):\n","    with open(filename, 'r') as f:\n","        data = [line.split('\\t') for line in f.read().splitlines()]\n","    return data\n","\n","def tokenize(doc):\n","  # norm, stem은 optional\n","  return ['/'.join(t) for t in twitter.pos(doc, norm=True, stem=True)]\n","\n","\n","\n","#doc2vec parameters\n","cores = multiprocessing.cpu_count()\n","\n","vector_size = 300\n","window_size = 15\n","word_min_count = 2\n","sampling_threshold = 1e-5\n","negative_size = 5\n","train_epoch = 100\n","dm = 1\n","worker_count = cores\n","\n","\n","# 트래이닝 데이터 읽기\n","train_data = read_data('/content/drive/My Drive/Colab Notebooks/Final_project/data/output/contents_text.txt')\n","\n","# 형태소 분류\n","train_docs = [(tokenize(row[1]), row[2]) for row in train_data[1:]]\n","\n","# doc2vec 에서 필요한 데이터 형식으로 변경\n","TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n","tagged_train_docs = [TaggedDocument(d, [c]) for d, c in train_docs]\n","\n","# 사전 구축\n","doc_vectorizer = doc2vec.Doc2Vec(size=300, alpha=0.025, min_alpha=0.025, seed=1234)\n","doc_vectorizer.build_vocab(tagged_train_docs)\n","\n","# Train document vectors!\n","for epoch in range(10):\n","    doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n","    doc_vectorizer.alpha -= 0.002  # decrease the learning rate\n","    doc_vectorizer.min_alpha = doc_vectorizer.alpha  # fix the learning rate, no decay\n","\n","#To save\n","doc_vectorizer.save('model/doc2vec.model')\n","\n","pprint(doc_vectorizer.most_similar('공포/Noun'))\n","pprint(doc_vectorizer.similarity('공포/Noun', 'ㅋㅋ/KoreanParticle'))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kr4i98CqL04z","colab_type":"code","colab":{}},"source":["## doc2vec_test\n","\n","from collections import namedtuple\n","from gensim.models import doc2vec\n","from konlpy.tag import Twitter\n","import multiprocessing\n","from pprint import pprint\n","from gensim.models import Doc2Vec\n","from sklearn.linear_model import LogisticRegression\n","import numpy\n","import pickle\n","\n","\n","twitter = Twitter()\n","\n","def read_data(filename):\n","    with open(filename, 'r') as f:\n","        data = [line.split('\\t') for line in f.read().splitlines()]\n","    return data\n","\n","def tokenize(doc):\n","  # norm, stem은 optional\n","  return ['/'.join(t) for t in twitter.pos(doc, norm=True, stem=True)]\n","\n","\n","# 테스트 데이터를 읽기\n","train_data = read_data('data/ratings_train.txt')\n","test_data = read_data('data/ratings_test.txt')\n","\n","# 형태소 분류\n","train_docs = [(tokenize(row[1]), row[2]) for row in train_data[1:]]\n","test_docs = [(tokenize(row[1]), row[2]) for row in test_data[1:]]\n","\n","# doc2vec 에서 필요한 데이터 형식으로 변경\n","TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n","tagged_train_docs = [TaggedDocument(d, [c]) for d, c in train_docs]\n","tagged_test_docs = [TaggedDocument(d, [c]) for d, c in test_docs]\n","\n","# load train data\n","doc_vectorizer = Doc2Vec.load('model/doc2vec.model')\n","\n","# 분류를 위한 피쳐 생성\n","train_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_train_docs]\n","train_y = [doc.tags[0] for doc in tagged_train_docs]\n","test_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_test_docs]\n","test_y = [doc.tags[0] for doc in tagged_test_docs]\n","\n","\n","#classifier = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)\n","classifier = LogisticRegression(random_state=1234)\n","classifier.fit(train_x, train_y)\n","\n","# 테스트 socre 확인\n","print( classifier.score(test_x, test_y) )\n","# 0.63904\n","\n","# save the model to disk\n","filename = 'model/finalized_model.sav'\n","pickle.dump(classifier, open(filename, 'wb'))"],"execution_count":0,"outputs":[]}]}